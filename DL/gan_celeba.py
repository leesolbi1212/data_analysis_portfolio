# âœ… 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import os
import zipfile
import urllib.request
from PIL import Image
import glob

# âœ… 2. ì •ìƒ ì‘ë™í•˜ëŠ” URLì—ì„œ CelebA Tiny (28x28, 2000ì¥) ë‹¤ìš´ë¡œë“œ
celeba_url = "https://github.com/inktokyo/public-datasets/releases/download/celeba-tiny/img_align_celeba_28x28.zip"
zip_path = "./celeba/img_align_celeba_28x28.zip"

if not os.path.exists(zip_path):
    print("ğŸ“¥ Downloading CelebA Tiny dataset...")
    urllib.request.urlretrieve(celeba_url, zip_path)

# âœ… 3. ì••ì¶• í•´ì œ
if not os.path.exists("./celeba/img_align_celeba"):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall("./celeba/")

# âœ… 4. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
def load_images(img_folder, img_size=(28, 28), max_images=2000):
    image_paths = glob.glob(os.path.join(img_folder, "*.jpg"))[:max_images]
    data = []
    for path in image_paths:
        img = Image.open(path).resize(img_size)
        img = np.asarray(img) / 127.5 - 1.0
        if img.shape == (28, 28, 3):
            data.append(img)
    return np.array(data, dtype=np.float32)

# âœ… 5. ë°ì´í„°ì…‹ ë¡œë”© ë° êµ¬ì„±
images = load_images("./celeba/img_align_celeba", max_images=2000)
dataset = tf.data.Dataset.from_tensor_slices(images).shuffle(2000).batch(256)

# âœ… 6. Generator ëª¨ë¸
def make_generator():
    model = tf.keras.Sequential([
        layers.Dense(128, activation='relu', input_shape=(100,)),
        layers.Dense(28 * 28 * 3, activation='tanh'),
        layers.Reshape((28, 28, 3))
    ])
    return model

# âœ… 7. Discriminator ëª¨ë¸
def make_discriminator():
    model = tf.keras.Sequential([
        layers.Flatten(input_shape=(28, 28, 3)),
        layers.Dense(128),
        layers.LeakyReLU(),
        layers.Dense(1, activation='sigmoid')
    ])
    return model

# âœ… 8. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì˜µí‹°ë§ˆì´ì €
generator = make_generator()
discriminator = make_discriminator()

cross_entropy = tf.keras.losses.BinaryCrossentropy()
gen_opt = tf.keras.optimizers.Adam(1e-4)
disc_opt = tf.keras.optimizers.Adam(1e-4)

# âœ… 9. ì†ì‹¤ í•¨ìˆ˜
def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    return real_loss + fake_loss

# âœ… 10. í•™ìŠµ Step ì •ì˜
@tf.function
def train_step(images):
    noise = tf.random.normal([256, 100])
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        fake_images = generator(noise, training=True)
        real_output = discriminator(images, training=True)
        fake_output = discriminator(fake_images, training=True)

        g_loss = generator_loss(fake_output)
        d_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)

    gen_opt.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    disc_opt.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

# âœ… 11. ì‹œê°í™” í•¨ìˆ˜
def generate_and_show_images(generator, epoch, seed):
    predictions = generator(seed, training=False)
    predictions = (predictions + 1) / 2.0

    plt.figure(figsize=(4, 4))
    for i in range(16):
        plt.subplot(4, 4, i + 1)
        plt.imshow(predictions[i])
        plt.axis("off")
    plt.suptitle(f"Epoch {epoch}")
    plt.tight_layout()
    plt.show()

# âœ… 12. í•™ìŠµ ë£¨í”„
def train(dataset, epochs):
    seed = tf.random.normal([16, 100])
    for epoch in range(epochs):
        for image_batch in dataset:
            train_step(image_batch)
        print(f"Epoch {epoch+1} ì™„ë£Œ âœ…")
        generate_and_show_images(generator, epoch+1, seed)

# âœ… 13. í•™ìŠµ ì‹œì‘
train(dataset, epochs=10)
